{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNn62mn9z6iv9fb+AhEpou",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saivinay997/Triton/blob/main/Triton_Day01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use T4 GPU\n",
        "\n",
        "Adding two tensors."
      ],
      "metadata": {
        "id": "Cm5rYr5OToeC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1_F7pjTTkbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ],
      "metadata": {
        "id": "KOj9kmlLcq6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def add_kernel(x_ptr, # pointer for the first input vector\n",
        "               y_ptr, # pointer for the second input vector\n",
        "               output_ptr, # pointer for the output vector\n",
        "               n_elements, # size of the vector\n",
        "               BLOCK_SIZE: tl.constexpr, # Number of elements each program should process (is it thread??)\n",
        "                # Note: 'constexpr' so it can be used as a shape value.\n",
        "               ):\n",
        "    # there are multiple `programs` processing different data.\n",
        "    # We identify which program we are in using program_id (pid)\n",
        "    pid = tl.program_id(axis=0) # We use a 1D launch grid, so axis is 0.\n",
        "    # This programm will process inputs that are offset from the initial data.\n",
        "    # For instance, if you had a vector length 256 and block_size of 64,\n",
        "    # the programms would each access the elements [0:64, 64:128, 128:192, 192:256]\n",
        "    # 1D grid of 4 block (since 256/64 = 4 blocks)\n",
        "    # Note that the offsets is a list of pointers.\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "    # Example of whats happening above. Consider 4 blocks as explaind above\n",
        "    # - 1st block (pid = 0)\n",
        "    # block_start = 0 * 64 = 0\n",
        "    # offsets = 0 + tl.arange(0, 64) --> [0, 1, 2, ... 63]\n",
        "    # this block processing elements [0:64]\n",
        "    #\n",
        "    # - 2nd block (pid = 1)\n",
        "    # block_start = 1 * 64 = 64\n",
        "    # offsets = 64 + tl.arange(0, 64) --> [64, 65, ... , 127]\n",
        "    # this block processing elements [64:128]\n",
        "    #\n",
        "    # so on...\n",
        "    # The entiere data arry is divided into chunks, with each block responsible for a range of 64 elements.\n",
        "    # The offsets list determines exactly which elements each block will process, based on its program ID(pid)\n",
        "\n",
        "    # Create mask to guard memory operations against out-of-bounds accesses.\n",
        "    mask = offsets < n_elements\n",
        "    # Load x and y from DRAM, masking out any extra elements in case the input is not a multiple of the blocksize\n",
        "    x = tl.load(x_ptr + offsets, mask=mask)\n",
        "    y = tl.load(y_ptr + offsets, mask=mask)\n",
        "    output = x + y\n",
        "    # output[i] = x[i] + y[i]\n",
        "\n",
        "    # write x + y back on DRAM\n",
        "    tl.store(output_ptr + offsets, output, mask=mask)\n",
        "\n",
        "\n",
        "def add(x: torch.Tensor, y: torch.Tensor):\n",
        "    # first we need to preallocate the tensor\n",
        "    output = torch.empty_like(x) # this is to store the output\n",
        "    assert x.is_cuda and y.is_cuda and output.is_cuda, \"Tensors are not on cuda (GPU). Set device to `cuda`\"\n",
        "    n_elements = output.numel()  # returns the total number of elements in the tensor [dim * rows * col]\n",
        "    # SPMD - Single Program Multiple Data, same program is executed across multiple processing usits, but each unit operates on separate subset of data.\n",
        "    # Launch grid - structure used to organize and coordinate the execution of the parallel program on GPUs, in this context\n",
        "    # The SPMD launch grid denots the number fo kernel instance that run in parallel\n",
        "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int]\n",
        "    # In this case, we use a 1D grid where the size is the number of blocks\n",
        "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),) # ceiling division ceil(a/b)\n",
        "    # Note:\n",
        "    # - Each torch.tensor object is implicitly converted into a pointer to its first element\n",
        "    # triton.jit - decorator in the Triton library used to define GPU kenrels that can be JIT(just-in-time) compiled.\n",
        "    # it converts Python functions into optimized GPU code at runtime, leveraging Triton's compiler\n",
        "    # - 'triton.jit' 'ed functions can be indexed with a launch grid to obtain a callable GPU kernel\n",
        "    # - Don't forget to pass meta-parameters as keywords arguments\n",
        "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
        "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called,\n",
        "    # the kernel is still running asynchronously at this point.\n",
        "    # What does the above comment mean:\n",
        "    # - GPU Operations (e.g., Kernel launches, memory transfers) are often asynchronous when called from the host (CPU) code.\n",
        "    # - When you invoke a kernel (e.g., a function marked with @triton.jit), it returns immediately\n",
        "    # while the GPU stars executing the operation in parallel\n",
        "    # - The code likely involves returning a variable (e.g., z), which is the result of a GPU computation\n",
        "    # - At the point of returning, the computation associated with z may not be finished because the kernel runs asynchronusly(not at the same time or speed)\n",
        "    # - `torch.cuda.synchronize()` function forces the CPU to wait until all previously launched GPU operations are completed.\n",
        "    # - Without this synchronization, the CPU may process with subsequent operations, even though the GPU is still processing the kernel\n",
        "    # - Synchronizing ensures that the result (z) is ready and valid before it is used for further computations or returned\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "Q36-BUhfA784"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "torch.manual_seed(0)\n",
        "size = 98432\n",
        "\n",
        "x = torch.rand(size, device=\"cuda\")\n",
        "y = torch.rand(size, device=\"cuda\")\n",
        "\n",
        "# Using torch\n",
        "start = time.time()\n",
        "output_torch = x + y\n",
        "end = time.time()\n",
        "print(f\"Time taken by pytorch cuda: {end-start}\")\n",
        "print(output_torch)\n",
        "start = time.time()\n",
        "\n",
        "#using Triton\n",
        "output_triton = add(x, y)\n",
        "end = time.time()\n",
        "print(f\"Time taken by triton: {end - start}\")\n",
        "print(output_triton)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1lWUe1icrzF",
        "outputId": "98c89a6a-9b54-441a-f196-318f7dd23b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken by pytorch cuda: 0.00016951560974121094\n",
            "tensor([1.3713, 1.3076, 0.4940,  ..., 0.4024, 1.7918, 1.0686], device='cuda:0')\n",
            "Time taken by triton: 0.0012385845184326172\n",
            "tensor([1.3713, 1.3076, 0.4940,  ..., 0.4024, 1.7918, 1.0686], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQNCk5u5dfBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}